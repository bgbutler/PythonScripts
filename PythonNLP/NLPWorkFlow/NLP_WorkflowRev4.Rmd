---
title: "NLP Workflow"
author: "Bryan Butler"
date: "3/9/2021"
output:
    html_document:
    toc: false
    toc_depth: 1
    fig_crop: no
---

```{r setup, include=FALSE}
library(knitr)
library(reticulate)
library(DT)

knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = FALSE, warning= FALSE, error =TRUE, message = FALSE, cache.lazy = FALSE, comment=NA, fig.width=10, fig.height=8)
use_condaenv('tensorflow2')


Sys.setenv(RETICULATE_PYTHON = '/Users/Bryan/anaconda/envs/tensorflow2/bin/python.app')
```


# {.tabset .tabset-fade .tabset-pills}


<style>
  .main-container {
    max-width: 1200px !important;
    margin-left: auto;
    margin-right: auto;
  }ibn
</style>

## About Me
### - Current role Head of Analytics & Data Science at Eastern Bank
### - BS & MS in Chemistry, MBA in Finance
### - Spent 15 years in the insurance/reinsurance industry modeling risk
### - Other roles in InsureTech, Banking, Marketing Research and Consulting
### - I like to give back at StackOverflow - I am a Necromancer
###
![](/Users/Bryan/Desktop/StackOverflow.png)

###
![](/Users/Bryan/Desktop/Necromancer_SO.png)


## Why Reticulate
### Reticulate = Python + RStudio
### Reticulate + RMarkdown + Plotly = Interactive Web Application
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/comic.png)

## Corporate Life
### Most Offices are Silos of Sad Cubicles
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/Sad_Cubicles_2.png)

###
### Leverage the Simplicity of Functional Programming with R
###
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/ggplot2_masterpiece.png)

###
### Your End Result
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/Office-R-github.png)

### Credit
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/Art_Allison_Horst.PNG)

## Reticulate Tips
### Setup
#### - Set up a separate Python environment with Anaconda (I call mine reticulate)
#### - Install PyQt5 in this environment; needed for rendering in a browser (it will break Spyder)
#### - Create and .Renviron file and put it in your home directory (use Sys.getenv() and look for HOME)
#### - Set the RETICULATE_PYTHON variable in the .Renviron file: RETICULATE_PYTHON="Path to your reticulate environment"
#### - RETICULATE_PYTHON="/Users/Bryan/anaconda/envs/tensorflow2/bin/" on my Mac
###
### While Using
#### - Need to use the print() command much more with Python, can't just call the object
#### - Sometimes you need to add plt.clf() before you plot to clear Matplotlib out (if using)
#### - Need to assign the plotting components to varaibles
![](/Users/Bryan/Documents/DSInfo/TextAnalytics/plottingR.png)





```{python importLibaries, echo=FALSE}
# Base libraries
# import PyQt5

# Standard imports
import sys
import warnings
warnings.simplefilter('ignore')
import os
import pandas as pd
import numpy as np
pd.pandas.set_option('display.max_columns', None)
pd.options.display.float_format  = '{:,.2f}'.format

pd.options.display.max_colwidth= 2000
pd.set_option('display.max_rows', 100)


# plotting
import textwrap
import matplotlib.pyplot as plt
import seaborn as sns

# improve with plotly
import plotly.offline as pyo
from plotly.offline import iplot, init_notebook_mode
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# get UMAP for dimension reduction
import umap


# for dendograms
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster import hierarchy

# get the lDA algorithm
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV

# text libraries
import nltk
from wordcloud import WordCloud, STOPWORDS# preprocessing prior to lda
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# use the TF-IDF vectorizor to give more weight to rare words
# TF-IDF specific setup
from sklearn.feature_extraction.text import TfidfVectorizer

# set seed for reproducibility
SEED = 42



```

```{python}

# print('Seaborn Version: ', sns.__version__)

```


```{python}
# TF imports
import tensorflow_hub as hub
import tensorflow as tf
```


```{python}
# Load the GUSE model

guse = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
```


```{python, echo=FALSE}
# get the data
# data = pd.read_csv('Transactio_4222021_240_PM.csv', low_memory=False)
data = pd.read_csv('/Users/Bryan/Documents/DSInfo/TextAnalytics/TestNLP.csv', low_memory=False)
```


```{python}
# regroup the OS

def get_os(row):
    if pd.isnull(row):
        return 'None'
    elif "Android" in row:
        return row[0:10]
    else:
        return row[0:7]
        
        
        
data['OS_Group'] = data['OS'].apply(get_os)

# remove the extra periods
data['OS_Group'] = data['OS_Group'].str.replace('.', '')

```


```{python}
text = data[['Please say more.', 'OS_Group']]

# rename the column
text.columns = ["Comments", 'OS']

# lots of NaN only get clean text
df = text[text['Comments'].notna()]

```

## Bigrams
### - Dendogram compares bigrams by OS
### - Distance/Adjacency reflects how similar phrases are
### - Adjacent OS contain similar bigrams
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# max_df - discard words that show up in x percent of documents has a scale 0 - 1
# min_df - is the opposite, minimum frequency can be a percent or raw number of documents
# ngram_range is normally (1,1) for single words, 2,2, for bigrams

cv = CountVectorizer(max_df = 0.95, min_df=2, stop_words='english', ngram_range=(2, 2))


# make document term matrix
dtm = cv.fit_transform(df['Comments'])

# convert to dataframe for clustering
bow = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names())


bow['OS'] = df['OS']

# collapse it down
osDf = bow.groupby(['OS']).sum()

# Calculate the distance between each sample
Z = linkage(osDf, 'ward')

# get the OS as a list
l = list(osDf.index)

# Make the dendrogram
# # Set the colour of the cluster here:
# hierarchy.set_link_color_palette(['#b30000','#996600', '#b30086'])
# colors = ["#2A66DE", "#E88202", "#2A66DE", "#E88202"]
hierarchy.set_link_color_palette(["darkblue", 'lightblue'])  

# set the threshold to color
t = 7.0

plt.figure(figsize=(14,8))
dend = dendrogram(Z, labels=l, leaf_rotation=0, leaf_font_size=12,
           orientation="left",
           color_threshold=t,
           above_threshold_color="orange")
plt.axvline(x=t, c='darkblue', lw=1, linestyle='dashed')
plt.title('Dendrogram of Phrases by OS')
plt.xlabel('OS')
plt.ylabel('Euclidean Distance')
plt.show();

```


## Topic Modeling
### - Different topics can contain the same phrases
### - Best separation of topics occcurs with 4 topics
### - Topics appear to be challenges, general feelings about eastern bank, app usage, experience with app
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# function to do a grid search of params
def cv_lda(data_in):
    search_params = {'n_components':[4, 6, 8], "learning_decay":[0.3, 0.5, 0.7]}
    
    # initialize model
    LDA = LatentDirichletAllocation(random_state = SEED)
    
    # init grid search class
    model = GridSearchCV(LDA, param_grid = search_params).fit(data_in)
    
    # return best model
    print("Best Model's Params: ", model.best_params_)
    print("\nBest Log Likelihood: ", model.best_score_)
    print("\nBest Perplexity: ", model.best_estimator_.perplexity(data_in))
    model.best_estimator_



LDA = LatentDirichletAllocation(n_components=4, random_state=SEED, learning_decay=0.3)
LDA.fit(dtm)

# get one topic
single_topic = LDA.components_[0]


# take a look
top_twenty_words = single_topic.argsort()[-20:]

# get thet top 20
for index in top_twenty_words:
    print(cv.get_feature_names()[index])

```



```{python, echo=FALSE, warning=FALSE, message=FALSE}
for index,topic in enumerate(LDA.components_):
    print(f'THE TOP 50 BIGRAMS WORDS FOR TOPIC #{index}')
    print([cv.get_feature_names()[i] for i in topic.argsort()[-50:]])
    print('\n')

topic_results = LDA.transform(dtm)


# get the first row, has probabilities of each topic
# topic_results[0].round(2)


# get index position of highest probability
# use it to assign the topic to the phrases

# topic_results[0].argmax()

# create a column for the topic assignment

df['Topic'] = topic_results.argmax(axis=1)

# map with words this is just a high level pass you'll have to study it more
mytopic_dict = {0:'challenges',1:'eastern bank',2:'usage',3:'experience',}

df['Topic Label'] = df['Topic'].map(mytopic_dict)

cleandf = df

```


### Dataframe Updated for Topics<br><br>

```{r, echo=FALSE}
# transfer it to R
rdf = reticulate::py$cleandf

datatable(rdf)


```




## Topic Plot
### - Dimension reduction allows for plotting in 2D for ease of viewing
### - Groupings were based off bigram similarities within a document
### - Colors represent the topics
```{python}
# Generate the TF-IDF vectors
# this is the same step as earlier with the count vectorizer
vectorizer_tfidf = TfidfVectorizer(max_features=10000, ngram_range = (2,2))
vectors_tfidf = vectorizer_tfidf.fit_transform(df.Comments)

# Generate the TF-IDF dimension reduction
embedding_tfidf = umap.UMAP(random_state=SEED).fit_transform(vectors_tfidf)

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
htmltools::includeHTML("topicBigrams.html")

```




## Sentiment Analysis
### - Splitting by Topic can Analyze the sentiment of each distinct Topic
### - Look for Topics that have Strong Sentiment
### - Sentiment is 75-80% accurate, currently exploring better options
```{python}
# instantiate sentiment engine
sid = SentimentIntensityAnalyzer()

# apply the sentiment analyzer
df['scores'] = df['Comments'].apply(lambda review: sid.polarity_scores(review))

# compound scores
df['compound'] = df['scores'].apply(lambda d:d['compound'])


# add some context
# you can choose scores fore neutral (-1 to 1 or something like that)
def sentiment_score(row):
    if row > .25:
        return 'Pos'
    elif row < -.25:
        return 'Neg'
    else:
        return "Neu"

df['sentiment'] = df['compound'].apply(sentiment_score)

newdf = df

ax = sns.displot(
    df, x="sentiment", col="Topic Label",
    binwidth=3, height=3, facet_kws=dict(margin_titles=True),
)
plt.show();
```


### Table Reflecting Values<br><br>
```{r, echo=FALSE}
# transfer it to R
rdf = reticulate::py$newdf

datatable(rdf)


```



## Phrase Cloud
### - Generated from Bigrams
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# get the phrases from the dtm 
sum_words = dtm.sum(axis=0) 
words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)

# set the stopwords list
stopwords = set(STOPWORDS)

# Generating wordcloud and saving as jpg image
plt.figure(figsize=(14,10))

words_dict = dict(words_freq)
WC_height = 1000
WC_width = 1500
WC_max_words = 500
wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,stopwords=stopwords)
word = wordCloud.generate_from_frequencies(words_dict)
# plt.title('Most frequently occurring bigrams connected by same color and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show();

```


## Deep Learning
### - Uses Google Universal Sentence Encoder (GUSE)
### - Handles the whole sentence, not just words or short phrases
### - Plot reflects comments and sentiment
### - Can toggle on and off topics by clicking on legend
```{python}
# convert main text field to a list
# make each column a separate list
text = df['Comments'].to_list()



# get the sentences
# np_list = np.asarray(sentence_list) and then convert it to tensor using
# tensor_list = tf.convert_to_tensor(np_list)
np_list = np.asarray(text)
sentences = tf.convert_to_tensor(np_list)

end = sentences.shape


# encode them all
vectors_guse = guse(sentences)

# Generate the GUSE dimension reductions
embedding_guse = umap.UMAP(random_state=SEED).fit_transform(vectors_guse)

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
htmltools::includeHTML("guse.html")

```

