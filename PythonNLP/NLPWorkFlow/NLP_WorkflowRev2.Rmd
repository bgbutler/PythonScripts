---
title: "NLP Workflow"
author: "Bryan Butler"
date: "3/9/2021"
output:
    html_document:
    toc: false
    toc_depth: 1
    fig_crop: no
---

```{r setup, include=FALSE}
library(knitr)
library(reticulate)
library(DT)
library(webshot)
webshot::install_phantomjs()
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(echo = FALSE, warning= FALSE, error =TRUE, message = FALSE, cache.lazy = FALSE, comment=NA, fig.width=10, fig.height=8)
use_condaenv('tensorflow2')


Sys.setenv(RETICULATE_PYTHON = '/Users/Bryan/anaconda/envs/tensorflow2/bin/python.app')
```


# {.tabset .tabset-fade .tabset-pills}


<style>
  .main-container {
    max-width: 1200px !important;
    margin-left: auto;
    margin-right: auto;
  }ibn
</style>


```{python importLibaries, echo=FALSE}
# Base libraries
# import PyQt5

# Standard imports
import sys
import warnings
warnings.simplefilter('ignore')
import os
import pandas as pd
import numpy as np
pd.pandas.set_option('display.max_columns', None)
pd.options.display.float_format  = '{:,.2f}'.format

pd.options.display.max_colwidth= 2000
pd.set_option('display.max_rows', 100)


# plotting
import textwrap
import matplotlib.pyplot as plt
import seaborn as sns

# improve with plotly
import plotly.offline as pyo
from plotly.offline import iplot, init_notebook_mode
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# get UMAP for dimension reduction
import umap


# for dendograms
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster import hierarchy

# get the lDA algorithm
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.model_selection import GridSearchCV

# text libraries
import nltk
from wordcloud import WordCloud, STOPWORDS# preprocessing prior to lda
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# use the TF-IDF vectorizor to give more weight to rare words
# TF-IDF specific setup
from sklearn.feature_extraction.text import TfidfVectorizer

# set seed for reproducibility
SEED = 42



```

```{python}

# print('Seaborn Version: ', sns.__version__)

```


```{python}
# TF imports
import tensorflow_hub as hub
import tensorflow as tf
```


```{python}
# Load the GUSE model

guse = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
```


```{python, echo=FALSE}
# get the data
# data = pd.read_csv('Transactio_4222021_240_PM.csv', low_memory=False)
data = pd.read_csv('/Users/Bryan/Documents/DSInfo/TextAnalytics/TestNLP.csv', low_memory=False)
```


```{python}
# regroup the OS

def get_os(row):
    if pd.isnull(row):
        return 'None'
    elif "Android" in row:
        return row[0:10]
    else:
        return row[0:7]
        
        
        
data['OS_Group'] = data['OS'].apply(get_os)

# remove the extra periods
data['OS_Group'] = data['OS_Group'].str.replace('.', '')

```


```{python}
text = data[['Please say more.', 'OS_Group']]

# rename the column
text.columns = ["Comments", 'OS']

# lots of NaN only get clean text
df = text[text['Comments'].notna()]

```

## Bigrams
### - Dendogram compares bigrams by OS
### - Distance/Adjacency reflects how similar phrases are
### - Adjacent OS contain similar bigrams
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# max_df - discard words that show up in x percent of documents has a scale 0 - 1
# min_df - is the opposite, minimum frequency can be a percent or raw number of documents
# ngram_range is normally (1,1) for single words, 2,2, for bigrams

cv = CountVectorizer(max_df = 0.95, min_df=2, stop_words='english', ngram_range=(2, 2))


# make document term matrix
dtm = cv.fit_transform(df['Comments'])

# convert to dataframe for clustering
bow = pd.DataFrame(dtm.toarray(), columns = cv.get_feature_names())


bow['OS'] = df['OS']

# collapse it down
osDf = bow.groupby(['OS']).sum()

# Calculate the distance between each sample
Z = linkage(osDf, 'ward')

# get the OS as a list
l = list(osDf.index)

# Make the dendrogram
# # Set the colour of the cluster here:
# hierarchy.set_link_color_palette(['#b30000','#996600', '#b30086'])
# colors = ["#2A66DE", "#E88202", "#2A66DE", "#E88202"]
hierarchy.set_link_color_palette(["darkblue", 'lightblue'])  

# set the threshold to color
t = 7.0

plt.figure(figsize=(14,8))
dend = dendrogram(Z, labels=l, leaf_rotation=0, leaf_font_size=12,
           orientation="left",
           color_threshold=t,
           above_threshold_color="orange")
plt.axvline(x=t, c='darkblue', lw=1, linestyle='dashed')
plt.title('Dendrogram of Phrases by OS')
plt.xlabel('OS')
plt.ylabel('Euclidean Distance')
plt.show();

```


## Topic Modeling
### - Different topics can contain the same phrases
### - Best separation of topics occcurs with 4 topics
### - Topics appear to be challenges, general feelings about eastern bank, app usage, experience with app
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# function to do a grid search of params
def cv_lda(data_in):
    search_params = {'n_components':[4, 6, 8], "learning_decay":[0.3, 0.5, 0.7]}
    
    # initialize model
    LDA = LatentDirichletAllocation(random_state = SEED)
    
    # init grid search class
    model = GridSearchCV(LDA, param_grid = search_params).fit(data_in)
    
    # return best model
    print("Best Model's Params: ", model.best_params_)
    print("\nBest Log Likelihood: ", model.best_score_)
    print("\nBest Perplexity: ", model.best_estimator_.perplexity(data_in))
    model.best_estimator_



LDA = LatentDirichletAllocation(n_components=4, random_state=SEED, learning_decay=0.3)
LDA.fit(dtm)

# get one topic
single_topic = LDA.components_[0]


# take a look
top_twenty_words = single_topic.argsort()[-20:]

# get thet top 20
for index in top_twenty_words:
    print(cv.get_feature_names()[index])

```



```{python, echo=FALSE, warning=FALSE, message=FALSE}
for index,topic in enumerate(LDA.components_):
    print(f'THE TOP 50 BIGRAMS WORDS FOR TOPIC #{index}')
    print([cv.get_feature_names()[i] for i in topic.argsort()[-50:]])
    print('\n')

topic_results = LDA.transform(dtm)


# get the first row, has probabilities of each topic
# topic_results[0].round(2)


# get index position of highest probability
# use it to assign the topic to the phrases

# topic_results[0].argmax()

# create a column for the topic assignment

df['Topic'] = topic_results.argmax(axis=1)

# map with words this is just a high level pass you'll have to study it more
mytopic_dict = {0:'challenges',1:'eastern bank',2:'usage',3:'experience',}

df['Topic Label'] = df['Topic'].map(mytopic_dict)

cleandf = df

```


### Dataframe Updated for Topics<br><br>

```{r, echo=FALSE}
# transfer it to R
rdf = reticulate::py$cleandf

datatable(rdf)


```




## Topic Plot
### - Dimension reduction allows for plotting in 2D for ease of viewing
### - Groupings were based off bigram similarities within a document
### - Colors represent the topics
```{python}
# Generate the TF-IDF vectors
# this is the same step as earlier with the count vectorizer
vectorizer_tfidf = TfidfVectorizer(max_features=10000, ngram_range = (2,2))
vectors_tfidf = vectorizer_tfidf.fit_transform(df.Comments)

# Generate the TF-IDF dimension reduction
embedding_tfidf = umap.UMAP(random_state=SEED).fit_transform(vectors_tfidf)


# set up plotting
df['x'] = embedding_tfidf[:,0]
df['y'] = embedding_tfidf[:,1]

# Wrap the text so it displays nicely in Plotly hover
df['wrap'] = df['Comments'].map(lambda x: '<br>'.join(textwrap.wrap(x, 64)))

colors = ['purple', 'orange', 'blue', 'green']
topics = list(df['Topic Label'].unique())



fig = go.Figure()


for i, topic in enumerate(topics):
    ix = df[df['Topic Label'] == topic]
    fig.add_trace(go.Scatter(x=ix.x,
                             y=ix.y,
                             name=topic,
                             showlegend=True,
                             mode='markers',
                             marker=dict(
                                 color=colors[i],
                                 size=4,
                                 opacity = 0.6)
                            ))


fig.update_traces(hovertemplate='OS: ' + df.OS + '<br>' + 'Topic: ' + df['Topic Label'] + '<br>' 
                  + df.wrap + '<extra></extra>')

   
fig.update_layout(
    autosize=False,
    width=1000,
    height=1000,
    hovermode="closest",
    title="Comment Clusters",
    legend_title_text='Topic',
    scene = dict(
        xaxis = dict(title=' ',visible=True, backgroundcolor="rgb(200, 200, 230)", tickvals=[]),
        yaxis = dict(title=' ',visible=True, backgroundcolor="rgb(200, 200, 230)", tickvals=[]),
    )
)


fig.update_layout(
    yaxis=dict(showticklabels=False),
    xaxis=dict(showticklabels=False)
)



 
    
fig.update_layout(legend=dict(
    orientation="h",
    yanchor="bottom",
    y=1.02,
    xanchor="right",
    x=1
))

    

topicplot = pyo.plot(fig, filename='topicBigrams.html', auto_open=False)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
htmltools::includeHTML("topicBigrams.html")

```






## Sentiment Analysis
### - Splitting by Topic can Analyze the sentiment of each distinct Topic
### - Look for Topics that have Strong Sentiment
### - Sentiment is 75-80% accurate, currently exploring better options
```{python}
# instantiate sentiment engine
sid = SentimentIntensityAnalyzer()

# apply the sentiment analyzer
df['scores'] = df['Comments'].apply(lambda review: sid.polarity_scores(review))

# compound scores
df['compound'] = df['scores'].apply(lambda d:d['compound'])


# add some context
# you can choose scores fore neutral (-1 to 1 or something like that)
def sentiment_score(row):
    if row > .25:
        return 'Pos'
    elif row < -.25:
        return 'Neg'
    else:
        return "Neu"

df['sentiment'] = df['compound'].apply(sentiment_score)

newdf = df

ax = sns.displot(
    df, x="sentiment", col="Topic Label",
    binwidth=3, height=3, facet_kws=dict(margin_titles=True),
)
plt.show();
```


### Table Reflecting Values<br><br>
```{r, echo=FALSE}
# transfer it to R
rdf = reticulate::py$newdf

datatable(rdf)


```



## Phrase Cloud
### - Generated from Bigrams
```{python, echo=FALSE, warning=FALSE, message=FALSE}
# get the phrases from the dtm 
sum_words = dtm.sum(axis=0) 
words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)

# set the stopwords list
stopwords = set(STOPWORDS)

# Generating wordcloud and saving as jpg image
plt.figure(figsize=(14,10))

words_dict = dict(words_freq)
WC_height = 1000
WC_width = 1500
WC_max_words = 500
wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,stopwords=stopwords)
word = wordCloud.generate_from_frequencies(words_dict)
# plt.title('Most frequently occurring bigrams connected by same color and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show();

```


## Deep Learning
### - Uses Google Universal Sentence Encoder (GUSE)
### - Handles the whole sentence, not just words or short phrases
### - Plot reflects comments and sentiment
### - Can toggle on and off topics by clicking on legend
```{python}
# convert main text field to a list
# make each column a separate list
text = df['Comments'].to_list()



# get the sentences
# np_list = np.asarray(sentence_list) and then convert it to tensor using
# tensor_list = tf.convert_to_tensor(np_list)
np_list = np.asarray(text)
sentences = tf.convert_to_tensor(np_list)

end = sentences.shape


# encode them all
vectors_guse = guse(sentences)

# Generate the GUSE dimension reductions
embedding_guse = umap.UMAP(random_state=SEED).fit_transform(vectors_guse)

# set up plotting
df['x'] = embedding_guse[:,0]
df['y'] = embedding_guse[:,1]
df['z'] = df['compound']


# Wrap the text so it displays nicely in Plotly hover
df['wrap'] = df['Comments'].map(lambda x: '<br>'.join(textwrap.wrap(x, 64)))

# prep for the loop
topics = list(df['Topic Label'].unique())
colors = ['purple', 'orange', 'blue', 'green']


fig = go.Figure()


for i, topic in enumerate(topics):
    ix = df[df['Topic Label'] == topic]
    fig.add_trace(go.Scatter3d(x=ix.x,
                               y=ix.y,
                               z=ix.compound,
                               name=topic,
                               showlegend=True,
                               mode='markers',
                               marker=dict(
                                   color=colors[i],
                                   size=4,
                                   opacity = 0.6,
                                   
                               )
                              ))


fig.update_traces(hovertemplate='OS: ' + df.OS + '<br>' + 'Topic: ' + df['Topic Label'] + '<br>' + 
                 'Sentiment: %{z:.2f}'+'<br>'+ df.wrap + '<extra></extra>')

   
fig.update_layout(
    autosize=False,
    width=1000,
    height=1000,
    hovermode="closest",
    legend_title_text='Topic',
    scene = dict(
        xaxis = dict(title=' ',visible=True, backgroundcolor="rgb(200, 200, 230)", tickvals=[]),
        yaxis = dict(title=' ',visible=True, backgroundcolor="rgb(200, 200, 230)", tickvals=[]),
        zaxis = dict(title='Sentiment',tickvals=[-1, -.75, -0.5, -0.25, 0, .25, 0.5,.75, 1.0])
    )
)
 
    
fig.update_layout(legend=dict(
    orientation="h",
    yanchor="bottom",
    y=.8,
    xanchor="right",
    x=1
))

    


guseplot = pyo.plot(fig, filename='guse.html', auto_open=False)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
htmltools::includeHTML("guse.html")

```

